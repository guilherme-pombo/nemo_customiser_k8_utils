{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d4c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from nemo_microservices import NeMoMicroservices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0b8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "# Initialize NeMo Microservices SDK client\n",
    "nemo_client = NeMoMicroservices(\n",
    "    base_url=NEMO_URL,\n",
    "    inference_base_url=NIM_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4cd9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: http://data-store.test\n",
      "Entity Store, Customizer, Evaluator endpoint: http://nemo.test\n",
      "NIM endpoint: http://nim.test\n",
      "Namespace: xlam-tutorial-ns\n",
      "Base Model for Customization: meta/llama-3.2-1b-instruct@v1.0.0+A100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store, Customizer, Evaluator endpoint: {NEMO_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model for Customization: {BASE_MODEL}@{BASE_MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275d979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where data preparation notebook saved finetuning and evaluation data\n",
    "DATA_ROOT = os.path.join(os.getcwd(), \"data\")\n",
    "# CUSTOMIZATION_DATA_ROOT = os.path.join(DATA_ROOT, \"customization\")\n",
    "# VALIDATION_DATA_ROOT = os.path.join(DATA_ROOT, \"validation\")\n",
    "# EVALUATION_DATA_ROOT = os.path.join(DATA_ROOT, \"evaluation\")\n",
    "\n",
    "# Sanity checks\n",
    "train_fp = f\"{DATA_ROOT}/train.jsonl\"\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "val_fp = f\"{DATA_ROOT}/val.jsonl\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "test_fp = f\"{DATA_ROOT}/test.jsonl\"\n",
    "assert os.path.exists(test_fp), f\"The test data at '{test_fp}' does not exist. Please ensure that the data was prepared successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a375bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_namespaces(nemo_client, ds_host, namespace):\n",
    "    # Create namespace in Entity Store\n",
    "    try:\n",
    "        namespace_obj = nemo_client.namespaces.create(id=namespace)\n",
    "        print(f\"Created namespace in Entity Store: {namespace_obj.id}\")\n",
    "    except Exception as e:\n",
    "        # Handle if namespace already exists\n",
    "        if \"409\" in str(e) or \"422\" in str(e):\n",
    "            print(f\"Namespace {namespace} already exists in Entity Store\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Create namespace in Data Store (still using requests as SDK doesn't cover Data Store)\n",
    "    nds_url = f\"{ds_host}/v1/datastore/namespaces\"\n",
    "    resp = requests.post(nds_url, data={\"namespace\": namespace})\n",
    "    assert resp.status_code in (200, 201, 409, 422), \\\n",
    "        f\"Unexpected response from Data Store during namespace creation: {resp.status_code}\"\n",
    "    print(f\"Data Store namespace creation response: {resp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b715021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created namespace in Entity Store: xlam-tutorial-ns\n",
      "Data Store namespace creation response: <Response [201]>\n"
     ]
    }
   ],
   "source": [
    "create_namespaces(nemo_client=nemo_client, ds_host=NDS_URL, namespace=NMS_NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a1b02f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store - Status Code: 201\n",
      "Response JSON: {'namespace': 'xlam-tutorial-ns', 'created_at': '2025-08-11T18:37:54Z', 'updated_at': '2025-08-11T18:37:54Z'}\n",
      "\n",
      "Entity Store - Namespace: xlam-tutorial-ns\n",
      "Created at: 2025-08-11 18:37:54.260547\n",
      "Description: None\n",
      "Project: None\n"
     ]
    }
   ],
   "source": [
    " # Verify Namespace in Data Store (using requests as SDK doesn't cover Data Store)\n",
    "response = requests.get(f\"{NDS_URL}/v1/datastore/namespaces/{NMS_NAMESPACE}\")\n",
    "print(f\"Data Store - Status Code: {response.status_code}\\nResponse JSON: {response.json()}\")\n",
    "\n",
    "# Verify Namespace in Entity Store\n",
    "namespace_obj = nemo_client.namespaces.retrieve(namespace_id=NMS_NAMESPACE)\n",
    "print(f\"\\nEntity Store - Namespace: {namespace_obj.id}\")\n",
    "print(f\"Created at: {namespace_obj.created_at}\")\n",
    "print(f\"Description: {namespace_obj.description}\")\n",
    "print(f\"Project: {namespace_obj.project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42b9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ffe235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RepoUrl('datasets/xlam-tutorial-ns/xlam-ft-dataset', endpoint='http://data-store.test/v1/hf', repo_type='dataset', repo_id='xlam-tutorial-ns/xlam-ft-dataset')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "hf_api = HfApi(endpoint=f\"{NDS_URL}/v1/hf\", token=\"\")\n",
    "\n",
    "# Create repo\n",
    "hf_api.create_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "830a69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.jsonl:   0%|          | 0.00/61.8M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.jsonl: 100%|██████████| 61.8M/61.8M [00:00<00:00, 422MB/s]\n",
      "val.jsonl: 100%|██████████| 15.4M/15.4M [00:00<00:00, 363MB/s]\n",
      "test.jsonl: 100%|██████████| 19.3M/19.3M [00:00<00:00, 363MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='', commit_message='Upload testing/test.jsonl with huggingface_hub', commit_description='', oid='fc6742ef802be72e0ab014568addf6bfada0eec7', pr_url=None, repo_url=RepoUrl('', endpoint='https://huggingface.co', repo_type='model', repo_id=''), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_api.upload_file(path_or_fileobj=train_fp,\n",
    "    path_in_repo=\"training/train.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "hf_api.upload_file(path_or_fileobj=val_fp,\n",
    "    path_in_repo=\"validation/val.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "hf_api.upload_file(path_or_fileobj=test_fp,\n",
    "    path_in_repo=\"testing/test.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dad47ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset: xlam-tutorial-ns/xlam-ft-dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(files_url='hf://datasets/xlam-tutorial-ns/xlam-ft-dataset', id='dataset-3nVot8VEHmjivMRoC9hg6Q', created_at=datetime.datetime(2025, 8, 11, 18, 38, 12, 94319), custom_fields={}, description='Tool calling xLAM dataset in OpenAI ChatCompletions format', format=None, hf_endpoint=None, limit=None, name='xlam-ft-dataset', namespace='xlam-tutorial-ns', project='tool_calling', split=None, updated_at=datetime.datetime(2025, 8, 11, 18, 38, 12, 94321))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Create dataset\n",
    "dataset = nemo_client.datasets.create(\n",
    "    name=DATASET_NAME,\n",
    "    namespace=NMS_NAMESPACE,\n",
    "    description=\"Tool calling xLAM dataset in OpenAI ChatCompletions format\",\n",
    "    files_url=f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}\",\n",
    "    project=\"tool_calling\",\n",
    ")\n",
    "print(f\"Created dataset: {dataset.namespace}/{dataset.name}\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674705b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files URL: hf://datasets/xlam-tutorial-ns/xlam-ft-dataset\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to validate dataset\n",
    "dataset_obj = nemo_client.datasets.retrieve(namespace=NMS_NAMESPACE, dataset_name=DATASET_NAME)\n",
    "\n",
    "print(\"Files URL:\", dataset_obj.files_url)\n",
    "assert dataset_obj.files_url == f\"hf://datasets/{repo_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10099b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created customization job: cust-AbFtZ5ABe9SoWa7wAxr1Re\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomizationJob(config='meta/llama-3.2-1b-instruct@v1.0.0+A100', dataset='xlam-tutorial-ns/xlam-ft-dataset', hyperparameters=Hyperparameters(finetuning_type='lora', batch_size=16, distillation=None, dpo=None, epochs=2, learning_rate=0.0001, log_every_n_steps=None, lora=LoraParameters(adapter_dim=32, adapter_dropout=0.1, alpha=16, target_modules=None), sequence_packing_enabled=False, sft=None, training_type='sft', val_check_interval=None, weight_decay=None), id='cust-AbFtZ5ABe9SoWa7wAxr1Re', config_snapshot=CustomizationConfigJobValue(base_model='meta/llama-3.2-1b-instruct', max_seq_length=4096, precision='bf16-mixed', training_option=CustomizationTrainingOption(finetuning_type='lora', micro_batch_size=1, num_gpus=1, training_type='sft', data_parallel_size=1, num_nodes=1, pipeline_parallel_size=1, tensor_parallel_size=1, use_sequence_parallel=False), dataset_schema=None, prompt_template='{prompt} {completion}'), created_at=datetime.datetime(2025, 8, 11, 18, 41, 0, 892174), dataset_parameters=None, description=None, integrations=None, namespace='default', output_model='xlam-tutorial-ns/llama-3.2-1b-xlam-run1@v1', ownership=None, project=None, status='created', status_details=CustomizationStatusDetails(created_at=datetime.datetime(2025, 8, 11, 18, 41, 1, 623969), status=None, updated_at=datetime.datetime(2025, 8, 11, 18, 41, 1, 623969), best_epoch=None, elapsed_time=0.0, epochs_completed=0, metrics=None, percentage_done=0.0, status_logs=[StatusLog(updated_at=datetime.datetime(2025, 8, 11, 18, 41, 1, 623969), detail=None, message='created')], steps_completed=0, steps_per_epoch=None, train_loss=None, val_loss=None), updated_at=datetime.datetime(2025, 8, 11, 18, 41, 0, 892176), warnings=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create customization job\n",
    "# If WANDB_API_KEY is set, we send it in the request header, which will report the training metrics to Weights & Biases (WandB).\n",
    "if WANDB_API_KEY:\n",
    "    client_with_wandb = nemo_client.with_options(default_headers={\"wandb-api-key\": WANDB_API_KEY})\n",
    "else:\n",
    "    client_with_wandb = nemo_client\n",
    "\n",
    "customization = client_with_wandb.customization.jobs.create(\n",
    "    name=\"llama-3.2-1b-xlam-ft\",\n",
    "    output_model=CUSTOM_MODEL,\n",
    "    config=f\"{BASE_MODEL}@{BASE_MODEL_VERSION}\",\n",
    "    dataset={\"name\": DATASET_NAME, \"namespace\": NMS_NAMESPACE},\n",
    "    hyperparameters={\n",
    "        \"training_type\": \"sft\",\n",
    "        \"finetuning_type\": \"lora\",\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"lora\": {\n",
    "            \"adapter_dim\": 32,\n",
    "            \"adapter_dropout\": 0.1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Created customization job: {customization.id}\")\n",
    "customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b99a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track status\n",
    "JOB_ID = customization.id\n",
    "\n",
    "customization = nemo_client.customization.jobs.retrieve(JOB_ID)\n",
    "\n",
    "# This will be the name of the model that will be used to send inference queries to\n",
    "CUSTOMIZED_MODEL = customization.output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b012883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage done: 0.0\n",
      "Job Status: {\n",
      "  \"created_at\": \"2025-08-11 18:05:51.648719\",\n",
      "  \"status\": \"running\",\n",
      "  \"updated_at\": \"2025-08-11 18:05:51.648719\",\n",
      "  \"best_epoch\": null,\n",
      "  \"elapsed_time\": 0.0,\n",
      "  \"epochs_completed\": 0,\n",
      "  \"metrics\": null,\n",
      "  \"percentage_done\": 0.0,\n",
      "  \"status_logs\": [\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:05:51\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"PVCCreated\"\n",
      "    },\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:05:51\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"EntityHandler_0_Created\"\n",
      "    },\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:05:51.648719\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"created\"\n",
      "    },\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:06:01\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"EntityHandler_0_Pending\"\n",
      "    },\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:06:01\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"EntityHandler_0_Completed\"\n",
      "    },\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:06:01\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"TrainingJobCreated\"\n",
      "    },\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:06:01\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"TrainingJobPending\"\n",
      "    },\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-11 18:07:01\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"NotEnoughResources\"\n",
      "    }\n",
      "  ],\n",
      "  \"steps_completed\": 0,\n",
      "  \"steps_per_epoch\": null,\n",
      "  \"train_loss\": null,\n",
      "  \"val_loss\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    " # Get job status\n",
    "job_status = nemo_client.customization.jobs.status(job_id=JOB_ID)\n",
    "\n",
    "print(\"Percentage done:\", job_status.percentage_done)\n",
    "print(\"Job Status:\", json.dumps(job_status.model_dump(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2e55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.11 seconds. Progress: 0.0%\n",
      "Job status: running after 10.13 seconds. Progress: 0.0%\n",
      "Job status: running after 15.27 seconds. Progress: 0.0%\n",
      "Job status: running after 20.28 seconds. Progress: 0.0%\n",
      "Job status: running after 25.30 seconds. Progress: 0.0%\n",
      "Job status: running after 30.56 seconds. Progress: 0.0%\n",
      "Job status: running after 35.58 seconds. Progress: 0.0%\n",
      "Job status: running after 40.60 seconds. Progress: 0.0%\n",
      "Job status: running after 45.74 seconds. Progress: 0.0%\n",
      "Job status: running after 50.76 seconds. Progress: 0.0%\n",
      "Job status: running after 55.78 seconds. Progress: 0.0%\n",
      "Job status: running after 60.92 seconds. Progress: 0.0%\n",
      "Job status: running after 65.94 seconds. Progress: 0.0%\n",
      "Job status: running after 70.96 seconds. Progress: 0.0%\n",
      "Job status: running after 76.13 seconds. Progress: 0.0%\n",
      "Job status: running after 81.15 seconds. Progress: 0.0%\n",
      "Job status: running after 86.16 seconds. Progress: 0.0%\n",
      "Job status: running after 91.31 seconds. Progress: 0.0%\n",
      "Job status: running after 96.33 seconds. Progress: 0.0%\n",
      "Job status: running after 101.35 seconds. Progress: 0.0%\n",
      "Job status: running after 106.59 seconds. Progress: 0.0%\n",
      "Job status: running after 111.61 seconds. Progress: 0.0%\n",
      "Job status: running after 116.63 seconds. Progress: 0.0%\n",
      "Job status: running after 121.77 seconds. Progress: 0.0%\n",
      "Job status: running after 126.79 seconds. Progress: 0.0%\n",
      "Job status: running after 131.81 seconds. Progress: 0.0%\n",
      "Job status: running after 136.96 seconds. Progress: 0.0%\n",
      "Job status: running after 141.97 seconds. Progress: 0.0%\n",
      "Job status: running after 146.99 seconds. Progress: 0.0%\n",
      "Job status: running after 152.16 seconds. Progress: 0.0%\n",
      "Job status: running after 157.18 seconds. Progress: 0.0%\n",
      "Job status: running after 162.20 seconds. Progress: 0.0%\n",
      "Job status: running after 167.43 seconds. Progress: 0.0%\n",
      "Job status: running after 172.45 seconds. Progress: 0.0%\n",
      "Job status: running after 177.46 seconds. Progress: 0.0%\n",
      "Job status: running after 182.61 seconds. Progress: 0.0%\n",
      "Job status: running after 187.63 seconds. Progress: 0.0%\n",
      "Job status: running after 192.65 seconds. Progress: 0.0%\n",
      "Job status: running after 197.79 seconds. Progress: 0.0%\n",
      "Job status: running after 202.81 seconds. Progress: 0.0%\n",
      "Job status: running after 207.83 seconds. Progress: 0.0%\n",
      "Job status: running after 212.98 seconds. Progress: 0.0%\n",
      "Job status: running after 218.00 seconds. Progress: 0.0%\n",
      "Job status: running after 223.02 seconds. Progress: 0.0%\n",
      "Job status: running after 228.16 seconds. Progress: 0.0%\n",
      "Job status: running after 233.18 seconds. Progress: 0.0%\n",
      "Job status: running after 238.20 seconds. Progress: 0.0%\n",
      "Job status: running after 243.44 seconds. Progress: 0.0%\n",
      "Job status: running after 248.46 seconds. Progress: 0.0%\n",
      "Job status: running after 253.48 seconds. Progress: 0.0%\n",
      "Job status: running after 258.62 seconds. Progress: 0.0%\n",
      "Job status: running after 263.64 seconds. Progress: 0.0%\n",
      "Job status: running after 268.66 seconds. Progress: 0.0%\n",
      "Job status: running after 273.82 seconds. Progress: 0.0%\n",
      "Job status: running after 278.84 seconds. Progress: 0.0%\n",
      "Job status: running after 283.86 seconds. Progress: 0.0%\n",
      "Job status: running after 289.03 seconds. Progress: 0.0%\n",
      "Job status: running after 294.05 seconds. Progress: 0.0%\n",
      "Job status: running after 299.07 seconds. Progress: 0.0%\n",
      "Job status: running after 304.30 seconds. Progress: 0.0%\n",
      "Job status: running after 309.32 seconds. Progress: 0.0%\n",
      "Job status: running after 314.34 seconds. Progress: 0.0%\n",
      "Job status: running after 319.49 seconds. Progress: 0.0%\n",
      "Job status: running after 324.51 seconds. Progress: 0.0%\n",
      "Job status: running after 329.54 seconds. Progress: 0.0%\n",
      "Job status: running after 334.69 seconds. Progress: 0.0%\n",
      "Job status: running after 339.71 seconds. Progress: 0.0%\n",
      "Job status: running after 344.73 seconds. Progress: 0.0%\n",
      "Job status: running after 349.88 seconds. Progress: 0.0%\n",
      "Job status: running after 354.90 seconds. Progress: 0.0%\n",
      "Job status: running after 359.92 seconds. Progress: 0.0%\n",
      "Job status: running after 365.08 seconds. Progress: 0.0%\n",
      "Job status: running after 370.10 seconds. Progress: 0.0%\n",
      "Job status: running after 375.12 seconds. Progress: 0.0%\n",
      "Job status: running after 380.35 seconds. Progress: 0.0%\n",
      "Job status: failed after 385.37 seconds. Progress: 0.0%\n"
     ]
    }
   ],
   "source": [
    " # Add wait job function to wait for the customization job to complete\n",
    "\n",
    "from time import sleep, time\n",
    "\n",
    "def wait_job(nemo_client, job_id: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job using SDK.\"\"\"\n",
    "    start_time = time()\n",
    "    job = nemo_client.customization.jobs.retrieve(job_id=job_id)\n",
    "    status = job.status\n",
    "\n",
    "    while (status in [\"pending\", \"created\", \"running\"]):\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Took more than {timeout} seconds.\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        job = nemo_client.customization.jobs.retrieve(job_id=job_id)\n",
    "        status = job.status\n",
    "        progress = 0.0\n",
    "        if status == \"running\" and job.status_details:\n",
    "            progress = job.status_details.percentage_done or 0.0\n",
    "        elif status == \"completed\":\n",
    "            progress = 100\n",
    "\n",
    "        print(f\"Job status: {status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
    "\n",
    "\n",
    "    return job\n",
    "\n",
    "job = wait_job(nemo_client, JOB_ID, polling_interval=5, timeout=2400)\n",
    "\n",
    "# Wait for 2 minutes, because sometimes, the job is finished, but the finetuned model is not ready in NIM yet.\n",
    "sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fec54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
