{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d4c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from nemo_microservices import NeMoMicroservices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bd8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"]=\"e0dae5c27937567b0d7c792082d063b2d7f7eed6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44fe6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0b8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "\n",
    "# Initialize NeMo Microservices SDK client\n",
    "nemo_client = NeMoMicroservices(\n",
    "    base_url=NEMO_URL,\n",
    "    inference_base_url=NIM_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ae4def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncDefaultPagination[Project](object='list', data=[], sort='created_at', pagination=DefaultPaginationPagination(current_page_size=0, page=1, page_size=10, total_pages=0, total_results=0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nemo_client.projects.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b4cd9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: http://data-store.test\n",
      "Entity Store, Customizer, Evaluator endpoint: http://nemo.test\n",
      "NIM endpoint: http://nim.test\n",
      "Namespace: lora-tutorial-ns\n",
      "Base Model for Customization: meta/llama-3.2-1b-instruct@v1.0.0+A100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store, Customizer, Evaluator endpoint: {NEMO_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model for Customization: {BASE_MODEL}@{BASE_MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275d979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where data preparation notebook saved finetuning and evaluation data\n",
    "DATA_ROOT = os.path.join(os.getcwd(), \"data\")\n",
    "# CUSTOMIZATION_DATA_ROOT = os.path.join(DATA_ROOT, \"customization\")\n",
    "# VALIDATION_DATA_ROOT = os.path.join(DATA_ROOT, \"validation\")\n",
    "# EVALUATION_DATA_ROOT = os.path.join(DATA_ROOT, \"evaluation\")\n",
    "\n",
    "# Sanity checks\n",
    "train_fp = f\"{DATA_ROOT}/training.jsonl\"\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "val_fp = f\"{DATA_ROOT}/validation.jsonl\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "test_fp = f\"{DATA_ROOT}/test.jsonl\"\n",
    "assert os.path.exists(test_fp), f\"The test data at '{test_fp}' does not exist. Please ensure that the data was prepared successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a375bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_namespaces(nemo_client, ds_host, namespace):\n",
    "    # Create namespace in Entity Store\n",
    "    try:\n",
    "        namespace_obj = nemo_client.namespaces.create(id=namespace)\n",
    "        print(f\"Created namespace in Entity Store: {namespace_obj.id}\")\n",
    "    except Exception as e:\n",
    "        # Handle if namespace already exists\n",
    "        if \"409\" in str(e) or \"422\" in str(e):\n",
    "            print(f\"Namespace {namespace} already exists in Entity Store\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Create namespace in Data Store (still using requests as SDK doesn't cover Data Store)\n",
    "    nds_url = f\"{ds_host}/v1/datastore/namespaces\"\n",
    "    resp = requests.post(nds_url, data={\"namespace\": namespace})\n",
    "    assert resp.status_code in (200, 201, 409, 422), \\\n",
    "        f\"Unexpected response from Data Store during namespace creation: {resp.status_code}\"\n",
    "    print(f\"Data Store namespace creation response: {resp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b715021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace lora-tutorial-ns already exists in Entity Store\n",
      "Data Store namespace creation response: <Response [409]>\n"
     ]
    }
   ],
   "source": [
    "create_namespaces(nemo_client=nemo_client, ds_host=NDS_URL, namespace=NMS_NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a1b02f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store - Status Code: 201\n",
      "Response JSON: {'namespace': 'lora-tutorial-ns', 'created_at': '2025-08-13T14:28:28Z', 'updated_at': '2025-08-13T14:33:28Z'}\n",
      "\n",
      "Entity Store - Namespace: lora-tutorial-ns\n",
      "Created at: 2025-08-13 14:28:28.058716\n",
      "Description: None\n",
      "Project: None\n"
     ]
    }
   ],
   "source": [
    " # Verify Namespace in Data Store (using requests as SDK doesn't cover Data Store)\n",
    "response = requests.get(f\"{NDS_URL}/v1/datastore/namespaces/{NMS_NAMESPACE}\")\n",
    "print(f\"Data Store - Status Code: {response.status_code}\\nResponse JSON: {response.json()}\")\n",
    "\n",
    "# Verify Namespace in Entity Store\n",
    "namespace_obj = nemo_client.namespaces.retrieve(namespace_id=NMS_NAMESPACE)\n",
    "print(f\"\\nEntity Store - Namespace: {namespace_obj.id}\")\n",
    "print(f\"Created at: {namespace_obj.created_at}\")\n",
    "print(f\"Description: {namespace_obj.description}\")\n",
    "print(f\"Project: {namespace_obj.project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08285af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(id='lora-tutorial-ns', created_at=datetime.datetime(2025, 8, 13, 14, 28, 28, 58716), custom_fields={}, description=None, ownership=None, project=None, updated_at=datetime.datetime(2025, 8, 13, 14, 28, 28, 58719))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namespace_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e42b9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffe235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "409 Client Error: Conflict for url: http://data-store.test/v1/hf/api/repos/create\n\nYou already created this repo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: http://data-store.test/v1/hf/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m hf_api \u001b[38;5;241m=\u001b[39m HfApi(endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNDS_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/v1/hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create repo\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mhf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3755\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3752\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3754\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3755\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3757\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3758\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 409 Client Error: Conflict for url: http://data-store.test/v1/hf/api/repos/create\n\nYou already created this repo"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "hf_api = HfApi(endpoint=f\"{NDS_URL}/v1/hf\", token=\"\")\n",
    "\n",
    "# Create repo\n",
    "hf_api.create_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "830a69d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='', commit_message='Upload testing/test.jsonl with huggingface_hub', commit_description='', oid='a94c9b8bcf70efc7cf80c008a3c033450d2a246a', pr_url=None, repo_url=RepoUrl('', endpoint='https://huggingface.co', repo_type='model', repo_id=''), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_api.upload_file(path_or_fileobj=train_fp,\n",
    "    path_in_repo=\"training/train.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "hf_api.upload_file(path_or_fileobj=val_fp,\n",
    "    path_in_repo=\"validation/val.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")\n",
    "\n",
    "hf_api.upload_file(path_or_fileobj=test_fp,\n",
    "    path_in_repo=\"testing/test.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e461a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResponse(id='dataset-2ibXH5f1wRC1jzn1ws9sQu', deleted_at=None, message='Resource deleted successfully.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nemo_client.datasets.delete(dataset_name=DATASET_NAME, namespace=NMS_NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dad47ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset: lora-tutorial-ns/news-lora-dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(files_url='hf://datasets/lora-tutorial-ns/news-lora-dataset', id='dataset-MvfFbbDRxmLWS8nmc7WGMX', created_at=datetime.datetime(2025, 8, 13, 16, 41, 25, 865354), custom_fields={}, description='News Dataset for FSI Blueprint', format=None, hf_endpoint=None, limit=None, name='news-lora-dataset', namespace='lora-tutorial-ns', project='tool_calling', split=None, updated_at=datetime.datetime(2025, 8, 13, 16, 41, 25, 865357))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Create dataset\n",
    "dataset = nemo_client.datasets.create(\n",
    "    name=DATASET_NAME,\n",
    "    namespace=NMS_NAMESPACE,\n",
    "    description=\"News Dataset for FSI Blueprint\",\n",
    "    files_url=f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}\",\n",
    "    project=\"tool_calling\",\n",
    "    \n",
    ")\n",
    "print(f\"Created dataset: {dataset.namespace}/{dataset.name}\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674705b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files URL: hf://datasets/lora-tutorial-ns/news-lora-dataset\n"
     ]
    }
   ],
   "source": [
    "# Sanity check to validate dataset\n",
    "dataset_obj = nemo_client.datasets.retrieve(namespace=NMS_NAMESPACE, dataset_name=DATASET_NAME)\n",
    "\n",
    "print(\"Files URL:\", dataset_obj.files_url)\n",
    "assert dataset_obj.files_url == f\"hf://datasets/{repo_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f93578c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta/llama-3.2-1b-instruct@v1.0.0+A100'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{BASE_MODEL}@{BASE_MODEL_VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfcb4d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 configs\n",
      "Namespace: meta Config:llama-3.2-1b-instruct@v1.0.0+L40 - None\n",
      "Namespace: meta Config:llama-3.2-1b-instruct@v1.0.0+A100 - None\n"
     ]
    }
   ],
   "source": [
    "# List customization configs with filters\n",
    "configs = nemo_client.customization.configs.list(\n",
    "    page=1,\n",
    "    page_size=10,\n",
    "    sort=\"-created_at\",\n",
    "    filter={\n",
    "        \"training_type\": \"sft\",\n",
    "        \"finetuning_type\": \"lora\",\n",
    "        \"enabled\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Found {len(configs.data)} configs\")\n",
    "for config in configs.data:\n",
    "    print(f\"Namespace: {config.namespace} Config:{config.name} - {config.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fa0be350",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL = 'lora-tutorial-ns/llama-3.2-1b-xlam-run1@v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bba88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResponse(id='model-YUsHWp4sVSgpEBKzmW64JJ', deleted_at=None, message='Resource deleted successfully.')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nemo_client.models.delete(namespace= NMS_NAMESPACE,model_name='llama-3.2-1b-xlam-run1@v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "10099b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created customization job: cust-VJ7FRPk9FLuH6RyMLYDyMb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomizationJob(config='meta/llama-3.2-1b-instruct@v1.0.0+A100', dataset='lora-tutorial-ns/news-lora-dataset', hyperparameters=Hyperparameters(finetuning_type='lora', batch_size=16, distillation=None, dpo=None, epochs=2, learning_rate=0.0001, log_every_n_steps=None, lora=LoraParameters(adapter_dim=16, adapter_dropout=0.1, alpha=16, target_modules=None), sequence_packing_enabled=True, sft=None, training_type='sft', val_check_interval=None, weight_decay=None), id='cust-VJ7FRPk9FLuH6RyMLYDyMb', config_snapshot=CustomizationConfigJobValue(base_model='meta/llama-3.2-1b-instruct', max_seq_length=4096, precision='bf16-mixed', training_option=CustomizationTrainingOption(finetuning_type='lora', micro_batch_size=1, num_gpus=1, training_type='sft', data_parallel_size=1, num_nodes=1, pipeline_parallel_size=1, tensor_parallel_size=1, use_sequence_parallel=False), dataset_schema=None, prompt_template='{prompt} {completion}'), created_at=datetime.datetime(2025, 8, 13, 18, 17, 19, 29071), dataset_parameters=None, description=None, integrations=None, namespace='default', output_model='lora-tutorial-ns/llama-3.2-1b-xlam-run1@v5', ownership=None, project=None, status='created', status_details=CustomizationStatusDetails(created_at=datetime.datetime(2025, 8, 13, 18, 17, 19, 419953), status=None, updated_at=datetime.datetime(2025, 8, 13, 18, 17, 19, 419953), best_epoch=None, elapsed_time=0.0, epochs_completed=0, metrics=None, percentage_done=0.0, status_logs=[StatusLog(updated_at=datetime.datetime(2025, 8, 13, 18, 17, 19, 419953), detail=None, message='created')], steps_completed=0, steps_per_epoch=None, train_loss=None, val_loss=None), updated_at=datetime.datetime(2025, 8, 13, 18, 17, 19, 29074), warnings=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create customization job\n",
    "# If WANDB_API_KEY is set, we send it in the request header, which will report the training metrics to Weights & Biases (WandB).\n",
    "if WANDB_API_KEY:\n",
    "    client_with_wandb = nemo_client.with_options(default_headers={\"wandb-api-key\": WANDB_API_KEY})\n",
    "else:\n",
    "    client_with_wandb = nemo_client\n",
    "\n",
    "customization = client_with_wandb.customization.jobs.create(\n",
    "    name=\"llama-3.2-1b-xlam-ft-seq-packed\",\n",
    "    output_model=CUSTOM_MODEL,\n",
    "    config=f\"{BASE_MODEL}@{BASE_MODEL_VERSION}\",\n",
    "    dataset={\"name\": DATASET_NAME, \"namespace\": NMS_NAMESPACE},\n",
    "    \n",
    "    hyperparameters={\n",
    "        \"sequence_packing_enabled\": True,\n",
    "        \"training_type\": \"sft\",\n",
    "        \"finetuning_type\": \"lora\",\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"lora\": {\n",
    "            \"adapter_dim\": 16,\n",
    "            \"adapter_dropout\": 0.1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Created customization job: {customization.id}\")\n",
    "customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1b99a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To track status\n",
    "JOB_ID = customization.id\n",
    "\n",
    "customization = nemo_client.customization.jobs.retrieve(JOB_ID)\n",
    "\n",
    "# This will be the name of the model that will be used to send inference queries to\n",
    "CUSTOMIZED_MODEL = customization.output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8608c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nemo_client.customization.jobs.cancel(job_id=JOB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b012883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage done: 0.0\n",
      "Job Status: {\n",
      "  \"created_at\": \"2025-08-13 18:17:19.419953\",\n",
      "  \"status\": \"created\",\n",
      "  \"updated_at\": \"2025-08-13 18:17:19.419953\",\n",
      "  \"best_epoch\": null,\n",
      "  \"elapsed_time\": 0.0,\n",
      "  \"epochs_completed\": 0,\n",
      "  \"metrics\": null,\n",
      "  \"percentage_done\": 0.0,\n",
      "  \"status_logs\": [\n",
      "    {\n",
      "      \"updated_at\": \"2025-08-13 18:17:19.419953\",\n",
      "      \"detail\": null,\n",
      "      \"message\": \"created\"\n",
      "    }\n",
      "  ],\n",
      "  \"steps_completed\": 0,\n",
      "  \"steps_per_epoch\": null,\n",
      "  \"train_loss\": null,\n",
      "  \"val_loss\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    " # Get job status\n",
    "job_status = nemo_client.customization.jobs.status(job_id=JOB_ID)\n",
    "\n",
    "print(\"Percentage done:\", job_status.percentage_done)\n",
    "print(\"Job Status:\", json.dumps(job_status.model_dump(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: running after 5.25 seconds. Progress: 50.0%\n",
      "Job status: running after 10.32 seconds. Progress: 50.0%\n",
      "Job status: running after 15.39 seconds. Progress: 50.0%\n",
      "Job status: running after 20.56 seconds. Progress: 50.0%\n",
      "Job status: running after 25.63 seconds. Progress: 50.0%\n",
      "Job status: running after 30.70 seconds. Progress: 50.0%\n",
      "Job status: running after 35.76 seconds. Progress: 50.0%\n",
      "Job status: running after 40.83 seconds. Progress: 50.0%\n",
      "Job status: running after 45.90 seconds. Progress: 50.0%\n",
      "Job status: running after 50.96 seconds. Progress: 50.0%\n",
      "Job status: running after 56.03 seconds. Progress: 50.0%\n",
      "Job status: running after 61.10 seconds. Progress: 50.0%\n",
      "Job status: running after 66.17 seconds. Progress: 50.0%\n",
      "Job status: running after 71.24 seconds. Progress: 50.0%\n",
      "Job status: running after 76.31 seconds. Progress: 50.0%\n",
      "Job status: running after 81.48 seconds. Progress: 50.0%\n",
      "Job status: running after 86.54 seconds. Progress: 50.0%\n",
      "Job status: running after 91.61 seconds. Progress: 50.0%\n",
      "Job status: running after 96.68 seconds. Progress: 50.0%\n",
      "Job status: running after 101.85 seconds. Progress: 50.0%\n",
      "Job status: running after 106.91 seconds. Progress: 50.0%\n",
      "Job status: running after 111.98 seconds. Progress: 50.0%\n",
      "Job status: running after 117.16 seconds. Progress: 50.0%\n",
      "Job status: running after 122.22 seconds. Progress: 50.0%\n",
      "Job status: running after 127.29 seconds. Progress: 50.0%\n"
     ]
    }
   ],
   "source": [
    " # Add wait job function to wait for the customization job to complete\n",
    "\n",
    "from time import sleep, time\n",
    "\n",
    "def wait_job(nemo_client, job_id: str, polling_interval: int = 10, timeout: int = 6000):\n",
    "    \"\"\"Helper for waiting an eval job using SDK.\"\"\"\n",
    "    start_time = time()\n",
    "    job = nemo_client.customization.jobs.retrieve(job_id=job_id)\n",
    "    status = job.status\n",
    "\n",
    "    while (status in [\"pending\", \"created\", \"running\"]):\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Took more than {timeout} seconds.\")\n",
    "\n",
    "        # Sleep before polling again\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        # Fetch updated status and progress\n",
    "        job = nemo_client.customization.jobs.retrieve(job_id=job_id)\n",
    "        status = job.status\n",
    "        progress = 0.0\n",
    "        if status == \"running\" and job.status_details:\n",
    "            progress = job.status_details.percentage_done or 0.0\n",
    "        elif status == \"completed\":\n",
    "            progress = 100\n",
    "\n",
    "        print(f\"Job status: {status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
    "\n",
    "\n",
    "    return job\n",
    "\n",
    "job = wait_job(nemo_client, JOB_ID, polling_interval=5, timeout=2400)\n",
    "\n",
    "# Wait for 2 minutes, because sometimes, the job is finished, but the finetuned model is not ready in NIM yet.\n",
    "sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "829fec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 models in namespace lora-tutorial-ns:\n",
      "\n",
      "Model: llama-3.2-1b-xlam-run1@v4\n",
      "  Namespace: lora-tutorial-ns\n",
      "  Base Model: meta/llama-3.2-1b-instruct\n",
      "  Created: 2025-08-13 17:02:36.340413\n",
      "  Fine-tuning Type: lora\n",
      "\n",
      "Model: llama-3.2-1b-xlam-run1@v1\n",
      "  Namespace: lora-tutorial-ns\n",
      "  Base Model: meta/llama-3.2-1b-instruct\n",
      "  Created: 2025-08-13 14:33:29.640105\n",
      "  Fine-tuning Type: lora\n"
     ]
    }
   ],
   "source": [
    " # List models with filters\n",
    "models_page = nemo_client.models.list(\n",
    "    filter={\"namespace\": NMS_NAMESPACE},\n",
    "    sort=\"-created_at\"\n",
    ")\n",
    "\n",
    "# Print models information\n",
    "print(f\"Found {len(models_page.data)} models in namespace {NMS_NAMESPACE}:\")\n",
    "for model in models_page.data:\n",
    "    print(f\"\\nModel: {model.name}\")\n",
    "    print(f\"  Namespace: {model.namespace}\")\n",
    "    print(f\"  Base Model: {model.base_model}\")\n",
    "    print(f\"  Created: {model.created_at}\")\n",
    "    if model.peft:\n",
    "        print(f\"  Fine-tuning Type: {model.peft.finetuning_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6f8f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: lora-tutorial-ns/llama-3.2-1b-xlam-run1@v4\n",
      "Base Model: meta/llama-3.2-1b-instruct\n",
      "Status: upload_completed\n"
     ]
    }
   ],
   "source": [
    "# CUSTOMIZED_MODEL is constructed as `namespace/model_name`, so we need to extract the model name\n",
    "model = nemo_client.models.retrieve(namespace=NMS_NAMESPACE, model_name=CUSTOMIZED_MODEL.split(\"/\")[1])\n",
    "\n",
    "print(f\"Model: {model.namespace}/{model.name}\")\n",
    "print(f\"Base Model: {model.base_model}\")\n",
    "print(f\"Status: {model.artifact.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32ac0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Check if the custom LoRA model is hosted by NVIDIA NIM\n",
    "models = nemo_client.inference.models.list()\n",
    "model_names = [model.id for model in models.data]\n",
    "\n",
    "assert CUSTOMIZED_MODEL in model_names, \\\n",
    "    f\"Model {CUSTOMIZED_MODEL} not found\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "693b9b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7676 examples in the test set\n"
     ]
    }
   ],
   "source": [
    "def read_jsonl(file_path):\n",
    "    \"\"\"Reads a JSON Lines file and yields parsed JSON objects\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "test_data = list(read_jsonl(test_fp))\n",
    "\n",
    "print(f\"There are {len(test_data)} examples in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a1fbc147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 77 batches\n",
      "First batch has 100 messages\n",
      "Sample message from first batch: {'role': 'user', 'content': \"Given the following headline:\\n### START HEADLINE ###\\n\\nHearing Endo Int'l Held Talks to Sell Paladin Labs to Knight Therapeutics\\n\\n### END HEADLINE ###\\n\\nWhat event type best classifies it? Choose from the following list:\\n\\n-analyst rating\\n-price targets\\n-earnings\\n-labour related\\n-mergers and acquisitions\\n-dividends\\n-regulatory\\n-stock price movement\\n-credit ratings\\n-products-services\\n-product approval\\n-guidance\\n-other\\n\\nProvide only the event type putting it inside double square brackets and in a new line like:\\n[[label]]\\n\\n### START EVENT OUTPUT ###\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "def create_message_batches(data_list, batch_size=100):\n",
    "    \"\"\"\n",
    "    Creates batches of messages from a list of dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of dictionaries containing 'prompt' key\n",
    "        batch_size: Size of each batch (default: 100)\n",
    "    \n",
    "    Returns:\n",
    "        List of batches, where each batch contains message dictionaries\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(data_list), batch_size):\n",
    "        batch = data_list[i:i + batch_size]\n",
    "        \n",
    "        # Create messages for this batch\n",
    "        batch_messages = []\n",
    "        for test_sample in batch:\n",
    "            messages = {\"role\": \"user\", \"content\": test_sample['prompt']}\n",
    "            batch_messages.append(messages)\n",
    "        \n",
    "        batches.append(batch_messages)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Usage example:\n",
    "message_batches = create_message_batches(test_data, batch_size=100)\n",
    "\n",
    "print(f\"Created {len(message_batches)} batches\")\n",
    "print(f\"First batch has {len(message_batches[0])} messages\")\n",
    "print(f\"Sample message from first batch: {message_batches[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 messages\n",
      "Processed 200 messages\n",
      "Processed 300 messages\n",
      "Processed 400 messages\n",
      "Processed 500 messages\n",
      "Processed 600 messages\n",
      "Processed 700 messages\n",
      "Processed 800 messages\n",
      "Processed 900 messages\n",
      "Processed 1000 messages\n",
      "Processed 1100 messages\n",
      "Processed 1200 messages\n",
      "Processed 1300 messages\n",
      "Processed 1400 messages\n",
      "Processed 1500 messages\n",
      "Processed 1600 messages\n",
      "Processed 1700 messages\n",
      "Processed 1800 messages\n",
      "Processed 1900 messages\n",
      "Processed 2000 messages\n",
      "Processed 2100 messages\n",
      "Processed 2200 messages\n",
      "Processed 2300 messages\n",
      "Processed 2400 messages\n",
      "Processed 2500 messages\n",
      "Processed 2600 messages\n",
      "Processed 2700 messages\n",
      "Processed 2800 messages\n",
      "Processed 2900 messages\n",
      "Processed 3000 messages\n",
      "Processed 3100 messages\n",
      "Processed 3200 messages\n",
      "Processed 3300 messages\n",
      "Processed 3400 messages\n",
      "Processed 3500 messages\n",
      "Processed 3600 messages\n",
      "Processed 3700 messages\n",
      "Processed 3800 messages\n",
      "Processed 3900 messages\n",
      "Processed 4000 messages\n",
      "Processed 4100 messages\n",
      "Processed 4200 messages\n",
      "Processed 4300 messages\n",
      "Processed 4400 messages\n",
      "Processed 4500 messages\n",
      "Processed 4600 messages\n",
      "Processed 4700 messages\n",
      "Processed 4800 messages\n",
      "Processed 4900 messages\n",
      "Processed 5000 messages\n",
      "Processed 5100 messages\n",
      "Processed 5200 messages\n",
      "Processed 5300 messages\n",
      "Processed 5400 messages\n",
      "Processed 5500 messages\n",
      "Processed 5600 messages\n",
      "Processed 5700 messages\n",
      "Processed 5800 messages\n",
      "Processed 5900 messages\n",
      "Processed 6000 messages\n",
      "Processed 6100 messages\n",
      "Processed 6200 messages\n",
      "Processed 6300 messages\n",
      "Processed 6400 messages\n",
      "Processed 6500 messages\n",
      "Processed 6600 messages\n",
      "Processed 6700 messages\n",
      "Processed 6800 messages\n",
      "Processed 6900 messages\n",
      "Processed 7000 messages\n",
      "Processed 7100 messages\n",
      "Processed 7200 messages\n",
      "Processed 7300 messages\n",
      "Processed 7400 messages\n",
      "Processed 7500 messages\n",
      "Processed 7600 messages\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "c=0\n",
    "for batch in message_batches:\n",
    "    for message in batch:\n",
    "        completion = nemo_client.chat.completions.create(model = CUSTOMIZED_MODEL,\n",
    "                                            messages = [message],\n",
    "                                            temperature = 0.1,\n",
    "                                            top_p = 0.7,\n",
    "                                            max_tokens = 512,\n",
    "                                            stream = False\n",
    "                                            )\n",
    "        resp = completion.choices[0].message.content\n",
    "        responses.append(resp)\n",
    "        c += 1\n",
    "        if c % 500 == 0:\n",
    "            print(f\"Processed {c} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "71268686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean responses using lambda to remove square brackets\n",
    "responses = list(map(lambda x: x.replace('[', '').replace(']', ''), responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d49f1c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Given the following headline:\\n### START HEADLINE ###\\n\\nHearing Endo Int'l Held Talks to Sell Paladin Labs to Knight Therapeutics\\n\\n### END HEADLINE ###\\n\\nWhat event type best classifies it? Choose from the following list:\\n\\n-analyst rating\\n-price targets\\n-earnings\\n-labour related\\n-mergers and acquisitions\\n-dividends\\n-regulatory\\n-stock price movement\\n-credit ratings\\n-products-services\\n-product approval\\n-guidance\\n-other\\n\\nProvide only the event type putting it inside double square brackets and in a new line like:\\n[[label]]\\n\\n### START EVENT OUTPUT ###\\n\\n\",\n",
       " 'completion': 'mergers and acquisitions'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2ccfa951",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [i['completion'] for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da742a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyst rating',\n",
       " 'credit ratings',\n",
       " 'dividends',\n",
       " 'earnings',\n",
       " 'guidance',\n",
       " 'labour issues',\n",
       " 'mergers and acquisitions',\n",
       " 'no event',\n",
       " 'other',\n",
       " 'price targets',\n",
       " 'product approval',\n",
       " 'products-services',\n",
       " 'regulatory',\n",
       " 'stock price movement'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a5c25844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-earnings',\n",
       " 'Dividends',\n",
       " 'Earnings',\n",
       " 'Fitch Affirming',\n",
       " 'Fitch Affirms',\n",
       " 'Fitch Affirms Ratings on AmEx, Discover, SLM; Outlook Stables',\n",
       " 'Fitch Downgrades',\n",
       " 'Fitch Expects',\n",
       " 'Hearing',\n",
       " 'Hearing Jana',\n",
       " 'IPO',\n",
       " 'IPO Outlook For The Week: Boats, Fertility Solutions, Risk Management And Chinese Supplements',\n",
       " 'IPO Wrapup for the Week of April 7th, 2014',\n",
       " 'Mergers and acquisitions',\n",
       " 'Stock price movement',\n",
       " 'UPDATE',\n",
       " 'analysis',\n",
       " 'analysis rating',\n",
       " 'analyst rating',\n",
       " 'analyzer rating',\n",
       " 'credit ratings',\n",
       " 'dividends',\n",
       " 'earnings',\n",
       " 'event',\n",
       " 'event rating',\n",
       " 'event type',\n",
       " 'eventing',\n",
       " 'events-services',\n",
       " 'filing',\n",
       " 'guidance',\n",
       " 'guide',\n",
       " 'hearing',\n",
       " 'issue notification',\n",
       " 'issues',\n",
       " 'joint application development and marketing',\n",
       " 'joint venture',\n",
       " 'judge rating',\n",
       " 'jury awarding',\n",
       " 'jury verdict',\n",
       " 'label',\n",
       " 'labor issues',\n",
       " 'labour issues',\n",
       " 'license agreement',\n",
       " 'mergers and acquisitions',\n",
       " \"news corporation in hot water over hacking of murdered teen's phone\",\n",
       " 'policy',\n",
       " 'price targets',\n",
       " 'product approval',\n",
       " 'products-services',\n",
       " 'regulatory',\n",
       " 'settlement',\n",
       " 'settlements',\n",
       " 'stock price movement',\n",
       " 'update',\n",
       " 'updates'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ece775d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3f769509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-earnings': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Dividends': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Earnings': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Fitch Affirming': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Fitch Affirms': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Fitch Affirms Ratings on AmEx, Discover, SLM; Outlook Stables': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Fitch Downgrades': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Fitch Expects': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Hearing': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'Hearing Jana': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'IPO': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'IPO Outlook For The Week: Boats, Fertility Solutions, Risk Management And Chinese Supplements': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'IPO Wrapup for the Week of April 7th, 2014': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Mergers and acquisitions': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'Stock price movement': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'UPDATE': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'analysis': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'analysis rating': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'analyst rating': {'precision': 0.13722156095902058,\n",
       "  'recall': 0.9527744982290437,\n",
       "  'f1-score': 0.23989298454221164,\n",
       "  'support': 847.0},\n",
       " 'analyzer rating': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'credit ratings': {'precision': 1.0,\n",
       "  'recall': 0.05,\n",
       "  'f1-score': 0.09523809523809523,\n",
       "  'support': 40.0},\n",
       " 'dividends': {'precision': 0.9787234042553191,\n",
       "  'recall': 0.8440366972477065,\n",
       "  'f1-score': 0.9064039408866995,\n",
       "  'support': 109.0},\n",
       " 'earnings': {'precision': 0.7218137254901961,\n",
       "  'recall': 0.589,\n",
       "  'f1-score': 0.6486784140969163,\n",
       "  'support': 1000.0},\n",
       " 'event': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'event rating': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'event type': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'eventing': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'events-services': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'filing': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'guidance': {'precision': 0.30434782608695654,\n",
       "  'recall': 0.056451612903225805,\n",
       "  'f1-score': 0.09523809523809523,\n",
       "  'support': 496.0},\n",
       " 'guide': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'hearing': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'issue notification': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'issues': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'joint application development and marketing': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'joint venture': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'judge rating': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'jury awarding': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'jury verdict': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'label': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'labor issues': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'labour issues': {'precision': 0.16666666666666666,\n",
       "  'recall': 0.005494505494505495,\n",
       "  'f1-score': 0.010638297872340425,\n",
       "  'support': 182.0},\n",
       " 'license agreement': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'mergers and acquisitions': {'precision': 0.8300653594771242,\n",
       "  'recall': 0.546236559139785,\n",
       "  'f1-score': 0.6588845654993515,\n",
       "  'support': 465.0},\n",
       " \"news corporation in hot water over hacking of murdered teen's phone\": {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'no event': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 438.0},\n",
       " 'other': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 795.0},\n",
       " 'policy': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'price targets': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 992.0},\n",
       " 'product approval': {'precision': 0.75,\n",
       "  'recall': 0.36666666666666664,\n",
       "  'f1-score': 0.4925373134328358,\n",
       "  'support': 90.0},\n",
       " 'products-services': {'precision': 0.6027397260273972,\n",
       "  'recall': 0.06821705426356589,\n",
       "  'f1-score': 0.12256267409470752,\n",
       "  'support': 645.0},\n",
       " 'regulatory': {'precision': 0.35294117647058826,\n",
       "  'recall': 0.013605442176870748,\n",
       "  'f1-score': 0.026200873362445413,\n",
       "  'support': 441.0},\n",
       " 'settlement': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'settlements': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'stock price movement': {'precision': 0.5542168674698795,\n",
       "  'recall': 0.08098591549295775,\n",
       "  'f1-score': 0.141321044546851,\n",
       "  'support': 1136.0},\n",
       " 'update': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'updates': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n",
       " 'accuracy': 0.25377800937988537,\n",
       " 'macro avg': {'precision': 0.11225853180531839,\n",
       "  'recall': 0.06269243774761978,\n",
       "  'f1-score': 0.060308706996676315,\n",
       "  'support': 7676.0},\n",
       " 'weighted avg': {'precision': 0.3639260069662098,\n",
       "  'recall': 0.25377800937988537,\n",
       "  'f1-score': 0.20915939141955522,\n",
       "  'support': 7676.0}}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_true=true_labels,\n",
    "    y_pred=responses,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823c299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[label]]'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Name of your custom model is: {CUSTOMIZED_MODEL}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef763cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
