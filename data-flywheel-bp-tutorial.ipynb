{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Discover Cost-Efficient AI Customer Service Agents with NVIDIA Data Flywheel Blueprint\n",
    "[![ Click here to deploy.](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W)\n",
    "\n",
    "In this notebook, you will learn how to use the Data Flywheel Blueprint to continuously discover and promote more cost-efficient agents for an [AI virtual customer service assistant](https://build.nvidia.com/nvidia/ai-virtual-assistant-for-customer-service).\n",
    "\n",
    "### Data Flywheel Blueprint\n",
    "\n",
    "![Data Flywheel Blueprint](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/update-launchable/docs/images/data-flywheel-blueprint.png)\n",
    "\n",
    "\n",
    "### AI Virtual Assistant for Customer Service\n",
    "\n",
    "The primary customer service agent in the AI Virtual Assistant uses tool calling to route user queries to specialized assistants, including: \n",
    "\n",
    "- Product Q&A\n",
    "- Order status verification\n",
    "- Returns processing\n",
    "- Small talk and casual engagement\n",
    "\n",
    "These interactions generate logs and tool-calling data that you can use as both evaluation benchmarks and training data. In this tutorial, you'll use this information to drive the flywheel process, fine-tuning smaller LLMs (such as `meta/llama-3.2-1B-instruct`, `meta/llama-3.2-3B-instruct`, `meta/llama-3.1-8B-instruct`) to match accuracy of the currently deployed model (`meta/llama-3.3-70B-instruct`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing with the Blueprint\n",
    "\n",
    "The following diagram illustrates how admin tools and applications interact with the Data Flywheel Blueprint, which orchestrates logging, processing, and model management to enable continuous optimization.\n",
    "\n",
    "![Arch](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/main/notebooks/arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents \n",
    "\n",
    "0. [Data Flywheel Setup](#0)\n",
    "1. [Load Sample Data](#1)\n",
    "2. [Create a Flywheel Job](#2)\n",
    "3. [Monitor Job Status](#3)\n",
    "4. [Optional: Show Continuous Improvement](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"0\"></a>\n",
    "## Data Flywheel Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Set NGC API key following the instructions at [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#generating-api-key).\n",
    "\n",
    "**When creating the key, make sure Public API Enpoints are included in the services.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['NGC_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Clone the data flywheel repo and fetch data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step presents two options:\n",
    "* **Step 2 (Option 1) NVIDIA Brev Launchable Setup:** The instructions below apply **only** to users running this notebook via the Brev Launchable.\n",
    "  \n",
    "NVIDIA Brev is a developer-friendly platform that makes it easy to run, train, and deploy ML models on cloud GPUs without the hassle of setupâ€”it comes preloaded with Python, CUDA, and Docker so you can get started fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/NVIDIA-AI-Blueprints/data-flywheel.git\n",
    "cd data-flywheel\n",
    "sudo apt-get update && sudo apt-get install -y git-lfs\n",
    "git lfs install\n",
    "git-lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"data-flywheel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Set up paths and install python dependencies for notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "data_dir = project_root / \"data\"\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory changed to: {Path.cwd()}\")\n",
    "\n",
    "# user_site = Path.home() / \".local\" / \"lib\" / f\"python{sys.version_info.major}.{sys.version_info.minor}\" / \"site-packages\"\n",
    "# if str(user_site) not in sys.path:\n",
    "#     sys.path.append(str(user_site))\n",
    "#     print(f\"Added user site-packages to sys.path: {user_site}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user elasticsearch==8.17.2 pandas>=2.2.3 matplotlib==3.10.3 pydantic==2.11.3 pydantic-settings==2.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Update `config/config.yaml` to use remote LLM as judge. By default, the Data Flywheel Blueprint deploys `LLama-3.3-70B-instruct` locally for LLM as a judge, which requires 4 GPUs. But for the launchable, we will choose the remote LLM judge and use the `LLama-3.1-70B-instruct` NIM hosted on [build.nvidia.com](https://build.nvidia.com/meta/llama-3_3-70b-instruct).\n",
    "\n",
    "By default, only `Llama-3.2-1b-instruct` will be used in the flywheel but you can uncomment other models in the yaml file to include in the flywheel run. You can also change other config settings such as data split and training hyperparameters as desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "\n",
    "new_llm_block = dedent(\"\"\"\\\n",
    "llm_judge_config:\n",
    "  deployment_type: \"remote\"\n",
    "  url: \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "  model_name: \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "new_nims_block = dedent(\"\"\"\\\n",
    "nims:\n",
    "  - model_name: \"meta/llama-3.2-1b-instruct\"\n",
    "    model_type: \"llm\"\n",
    "    context_length: 8192\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "    customizer_configs:\n",
    "      target: \"meta/llama-3.2-1b-instruct@2.0\"\n",
    "      gpus: 1\n",
    "      max_seq_length: 8192\n",
    "\n",
    "\n",
    "  - model_name: \"meta/llama-3.2-3b-instruct\"\n",
    "    model_type: \"llm\"\n",
    "    context_length: 8192\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "    customizer_configs:\n",
    "      target: \"meta/llama-3.2-3b-instruct@2.0\"\n",
    "      gpus: 1\n",
    "      max_seq_length: 8192\n",
    "\n",
    "\n",
    "  - model_name: \"meta/llama-3.1-8b-instruct\"\n",
    "    model_type: \"llm\"\n",
    "    context_length: 8192\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "    customizer_configs:\n",
    "      target: \"meta/llama-3.1-8b-instruct@2.0\"\n",
    "      gpus: 1\n",
    "      max_seq_length: 8192\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "text = config_path.read_text()\n",
    "\n",
    "def replace_block(yaml_text: str, key: str, new_block: str) -> str:\n",
    "    pattern = rf\"(?ms)^({re.escape(key)}:[\\s\\S]*?)(?=^\\S|\\Z)\"\n",
    "    return re.sub(pattern, new_block, yaml_text)\n",
    "\n",
    "text = replace_block(text, \"llm_judge_config\", new_llm_block)\n",
    "text = replace_block(text, \"nims\",              new_nims_block)\n",
    "\n",
    "config_path.write_text(text)\n",
    "print(\"config.yaml updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use remote LLM as judge, we will set the API key to access the remote LLM. You can create an API Key at https://build.nvidia.com/settings/api-keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NVIDIA_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Start data flywheel service, which involves first deploying the Nemo Microservices and then bring up the data flywheel service via docker compose with MLFlow enabled. This step may take about 10 minutes.\n",
    "\n",
    "> **Note:** The `deploy-nmp.sh` script automates the deployment of NeMo Microservices. For manual setup or advanced configuration, please consult the [NeMo Microservices documentation](https://docs.nvidia.com/nemo/microservices/latest/get-started/platform-prereq.html#beginner-tutorial-prerequisites).\n",
    "\n",
    "If you choose to manually deploy the Nemo Microservices Platform, then make sure you update the `nmp_config` field in the `config/config.yaml` with the correct base urls. The default is:\n",
    "```\n",
    "nmp_config:\n",
    "      nemo_base_url: \"http://nemo.test\"\n",
    "      nim_base_url: \"http://nim.test\"\n",
    "      datastore_base_url: \"http://data-store.test\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"$NGC_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# set -e\n",
    "\n",
    "# log() {\n",
    "#   echo -e \"\\033[1;32m[INFO]\\033[0m $1\"\n",
    "# }\n",
    "\n",
    "# echo \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "# chmod +x scripts/deploy-nmp.sh\n",
    "# ./scripts/deploy-nmp.sh --progress\n",
    "# log \"Starting data flywheel service...\"\n",
    "# export COMPOSE_PROFILES=mlflow && docker compose -f deploy/docker-compose.yaml up -d --build >> flywheel_deploy.log 2>&1\n",
    "# log \"Data flywheel service started successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## Step 1: Load Sample Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import required libraries and configure pandas display options for better readability in notebook outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Width of the display in characters\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the provided sample dataset from AI Virtual Assistant (`aiva`) (`data/aiva_primary_assistant_dataset.jsonl`) to simulate real user logs captured while an agentic customer service agent application is running. Each data point has the following schema:\n",
    "\n",
    "| Field        | Type               | Description                                                         |\n",
    "|--------------|--------------------|---------------------------------------------------------------------|\n",
    "| `timestamp`  | `int` (epoch secs) | Time the request was issued                                         |\n",
    "| `workload_id`| `str`              | Stable identifier for the logical task / route / agent node         |\n",
    "| `client_id`  | `str`              | Identifier of the application or deployment that generated traffic  |\n",
    "| `request`    | `dict`             | Exact [`openai.ChatCompletion.create`](https://platform.openai.com/docs/api-reference/chat/create) payload received by the model |\n",
    "| `response`   | `dict`             | Exact `ChatCompletion` response returned by the model               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `request` uses the OpenAI `ChatCompletions` request format and contains the following attributes:\n",
    "\n",
    "- `model` includes the Model ID used to generate the response.\n",
    "- `messages` includes a `system` message as well as a `user` query.\n",
    "- `tools` includes a list of functions and parameters available to the LLM to choose from, as well as their parameters and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "NEWS_DATA_PATH = '/home/shadeform/nemo_customiser_k8_utils/data'\n",
    "dataset = datasets.load_dataset(NEWS_DATA_PATH)\n",
    "\n",
    "# Get class labels\n",
    "stratify_column_name = \"label\"\n",
    "dataset = dataset.class_encode_column(stratify_column_name)\n",
    "class_labels = dataset.features['label']\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset['train'])} examples\")\n",
    "print(f\"Classes: {class_labels.names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PROMPT_FORMAT = \"\"\"\n",
    "You are a helpful AI assistant that analyses financial news headlines and identifies what event type is described.\n",
    "You will classify event types into one of the following categories (in square brackets)\n",
    "\n",
    "- [Analyst Rating]: An entity such as a bank, asset manager, etc. gives a classification/rating/downgrade/upgrade/opinion to an asset.\n",
    "                    If there is no specified analyst and company given, it's not Analyst Rating and should be classified as OTHER.\n",
    "- [Price Targets]: A mention of a price target (PT) is given by an entity such as bank, asset manager, etc.\n",
    "                   This takes priority over any other class, so if a price target is present use this class!\n",
    "- [Earnings]: Reports of quarterly, monthly, etc. concrete values of revenue, ESP, etc. Percentage fluctations.\n",
    "              Expected values are not Earnings and should be Guidance instead!\n",
    "- [Labour Issues]: Mentions of layoffs, union action, strikes, rising cost of labour, bonuses for execs, etc. Important personal change, e.g. CEO, CFO, VPs, etc.\n",
    "- [Mergers and Acquisitions]: Whenever merging or acquisition of entities, not just companies, is mentioned.\n",
    "                              Things like partnerships do not belong to this class! Takes priority over other classes.\n",
    "- [Dividends]: Mentions of dividend performance, dividend per share, decisions not to issue dividends, etc.\n",
    "- [Regulatory]: Mentions corporate position focused on environmental affairs, government regulation, international treaties, geopolitics, debt repayment, licenses, patents, etc.\n",
    "                Any executive decisions taken by the government are Regulatory. Takes priority over other classes.\n",
    "- [Stock price movement]: Includes pricing of public offerings, daily, monthly and yearly movements, highs and lows, options trade and alerts, etc.\n",
    "                          Only relevant when a specific entity/industry or set of entities/industries mentioned.\n",
    "                          Quantities such as sales, are not stock price movements. Stock splits do not count as stock movements.\n",
    "                          Takes priority over Earnings, Stock Price Movement, Credit Ratings\n",
    "- [Credit Ratings]: Adjustments of company's borrowing capacity, changes in debt values, changes in ratings, etc.\n",
    "- [Products-Services]: Mentions of a company's particular product, forward-looking product directions, disruption, government and private contracts, etc.\n",
    "                       Any sort of delay regarding a product roll-out, etc.\n",
    "- [Product Approval]: Mentions FDA approvals, environmental approvals, acceptance for review. Any time an entity approves the rollout of a corporations product.\n",
    "- [Guidance]: Forward looking statements issued by the company's themselves regarding Revenue, EPS, potential sales going up/down, number of contracts, etc.\n",
    "              Similar talk to earnings but about projections rather than realised.\n",
    "              \n",
    "If the headline doesn't match any of the classes, classify it as OTHER.\n",
    "If there are no events at all described in the headline, classify it as NO EVENT.\n",
    "ATTENTION:\n",
    "  - Only assign a category if the headline meets all the criteria listed for that category. Otherwise use OTHER\n",
    "  - Encourage precise matching rather than assigning categories based on partial or superficial similarities\n",
    "  - OTHER is the default category for when there is an event but there is doubt about which type\n",
    "  - If there are no events mentioned, use NO EVENT\n",
    "\n",
    "A few examples:\n",
    "\n",
    "1. Cornerstone OnDemand Higher as Barclays Upgraded to Overweight -> [Analyst Rating]\n",
    "2. HC Wainwright & Co. Maintains Buy on Balchem, Lowers Price Target to $104 -> [Price Targets]\n",
    "3. Sina Reports Q4 Adj. EPS $0.24 vs $0.18 Est., Sales $211.1M vs $207.6M Est.; Sees FY15 Sales $800M-$900M vs $884.6M Est. -> [Earnings]\n",
    "4. Delta To Buy Out Employees, Offer Early Retirement (DAL) -> [Labour Issues]\n",
    "5. Constant Contact Announces Deal to Be Purchased by Endurance Int'l at $32/Share -> [Mergers and Acquisitions]\n",
    "6. Ameriprise Financial Announces 12% Qtr. Dividend Increase to $0.58/Share -> [Dividends]\n",
    "7. Ultratech Achieves ISO 9001 and 14001 Certification for Singapore Operations and Recertification for U.S. Facility  -> [Regulatory]\n",
    "8. Mid-Afternoon Market Update: Dow Up Over 200 Points; Lakeland Industries Shares Spike Higher -> [Stock price movement]\n",
    "9. Moody's Affirms Ratings on Nokia; Outlook Revised from Negative to Developing -> [Credit Ratings]\n",
    "10. NOVAVAX Awarded HHS-BARDA Contract Valued at up to $179 Million  -> [Products-Services]\n",
    "11. Sanofi's Genzyme Announces Lemtrada Resubmission Accepted for Review by FDA  -> [Product Approval]\n",
    "12. Dot Hill Systems Announces Re-alignment of Software Development Initiatives; Sees Q4 EPS of $(0.02)-(0.03) vs $0.03-(0.03) Prior  -> [Guidance]\n",
    "13. Kopin Chairman Fan Buys 116,400 Shares @$2.83/Share -Form 4 -> [OTHER]\n",
    "\n",
    "Given the following headline:\n",
    "\n",
    "### START HEADLINE ###\n",
    "\n",
    "{headline}\n",
    "\n",
    "### END HEADLINE ###\n",
    "\n",
    "What event type best classifies it? Answer only with your predicted class and give it inside double square brackets, like [[class]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to flywheel format (prompt-completion pairs with metadata)\n",
    "def convert_to_flywheel_format(dataset_split, workload_id, client_id):\n",
    "    \"\"\"\n",
    "    Convert news classification dataset to flywheel log format\n",
    "    \"\"\"\n",
    "    flywheel_data = []\n",
    "    \n",
    "    for idx, example in enumerate(dataset_split):\n",
    "        headline = example[\"headline\"]\n",
    "        label = class_labels.int2str(example[\"label\"])\n",
    "        \n",
    "        # Create request in chat completion format\n",
    "        request = {\n",
    "            \"model\": \"meta/llama-3.3-70b-instruct\",  # Base model used for ground truth\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a financial news classifier.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": DATASET_PROMPT_FORMAT.format(headline=headline)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Create response with ground truth completion\n",
    "        response = {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": f\"[[{label}]]\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Create flywheel log entry\n",
    "        log_entry = {\n",
    "            \"request\": request,\n",
    "            \"response\": response,\n",
    "            \"workload_id\": workload_id,\n",
    "            \"client_id\": client_id,\n",
    "            \"timestamp\": int(time.time()) + idx\n",
    "        }\n",
    "        \n",
    "        flywheel_data.append(log_entry)\n",
    "    \n",
    "    return flywheel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "seed = 42\n",
    "\n",
    "dataset_train = dataset.get('train')\n",
    "split_dataset = dataset_train.train_test_split(\n",
    "    test_size=val_ratio + test_ratio, \n",
    "    seed=seed, \n",
    "    stratify_by_column='label'\n",
    ")\n",
    "split_dataset2 = split_dataset['test'].train_test_split(\n",
    "    test_size=test_ratio / (val_ratio + test_ratio), \n",
    "    seed=seed,\n",
    "    stratify_by_column='label'\n",
    ")\n",
    "\n",
    "# Create different client datasets with varying sizes for continuous improvement demo\n",
    "client_datasets = {\n",
    "    \"news-classifier-300\": split_dataset['train'].select(range(300)),\n",
    "    \"news-classifier-500\": split_dataset['train'].select(range(500)),\n",
    "    \"news-classifier-1000\": split_dataset['train'].select(range(1000)),\n",
    "}\n",
    "\n",
    "print(\"Dataset splits created:\")\n",
    "for client_id, ds in client_datasets.items():\n",
    "    print(f\"  {client_id}: {len(ds)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets in flywheel format\n",
    "FLYWHEEL_DATA_DIR = data_dir / \"news_classification\"\n",
    "FLYWHEEL_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for client_id, ds in client_datasets.items():\n",
    "    flywheel_data = convert_to_flywheel_format(\n",
    "        ds, \n",
    "        workload_id=\"news_classifier\",\n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    output_file = FLYWHEEL_DATA_DIR / f\"{client_id}_dataset.jsonl\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for entry in flywheel_data:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(flywheel_data)} examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points generated by AI Virtual Assistant in response to user queries are considered **ground truth**. \n",
    "\n",
    "Ground truth data points are used to **evaluate** and **customize** more efficient models that can perform similarly to the current model. This customization process is analogous to a student-teacher distillation setup, where synthetic data generated from the teacher model is used to fine-tune a student model.\n",
    "\n",
    "Next, we'll load the data into Elasticsearch using a helper method `load_data_to_elasticsearch`, making it accessible to the Data Flywheel service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Elasticsearch\n",
    "from src.scripts.load_test_data import load_data_to_elasticsearch\n",
    "\n",
    "# Start with the smallest dataset\n",
    "DATA_PATH = FLYWHEEL_DATA_DIR / \"news-classifier-300_dataset.jsonl\"\n",
    "load_data_to_elasticsearch(file_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"2\"></a>\n",
    "## Step 2: Create a Flywheel Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a Flywheel job by sending a POST request to the `/jobs` API. This triggers the workflow asynchronously.\n",
    "\n",
    "In production environments, you can automate this process to run at scheduled intervals, in response to specific events, or on demand.\n",
    "\n",
    "For this tutorial, we will target the primary customer service agent by setting the `workload_id` to \"primary_assistant\" and we will set `client_id` to \"aiva-1\" which has 300 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"news_classifier\", \"client_id\": \"news-classifier-300\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each candidate model, the data flywheel runs evaluations on the base model and its in-context learning (ICL) variant. If customization is enabled, the model is fine-tuned and evaluated again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"3\"></a>\n",
    "## Step 3: Monitor Job Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a GET request to `/jobs/{job_id}` to retrieve the current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(job_id):\n",
    "    \"\"\"Get the current status of a job.\"\"\"\n",
    "    response = requests.get(f\"{API_BASE_URL}/api/jobs/{job_id}\")\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_status(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the job status output, you will see the following metrics for evaluating the accuracy of tool calling once evaluations get completed:\n",
    "\n",
    "| Metric Name                                   | Definition                                                                                                         | Scoring Criteria                                                                                                         | Notes                                                                                                                        |\n",
    "|------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Function name accuracy**                         | Checks if the predicted function name exactly matches the ground truth function name.                              | 1 if predicted function name is an exact match; 0 otherwise.                                                             | Evaluates only the function name, not arguments.                                                                             |\n",
    "| **Function name + args accuracy (exact-match)**  | Checks if both the function name and all arguments exactly match the ground truth.                                  | 1 if both function name and all arguments are exact matches; 0 otherwise.                                                | Strictest metric; all parts must match exactly.                                                                              |\n",
    "| **Function name + args accuracy (LLM-judge)**    | Checks if the function name matches exactly, and arguments are either exact matches or semantically equivalent.     | 1 if function name matches and each argument is either an exact match or semantically correct (as judged by an LLM); 0 otherwise. | Allows semantic similarity for complex arguments; captures intent and functional correctness even with paraphrasing.          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process and enable continuous monitoring, we defined a utility function `monitor_job` in `utils/job_monitor_helper.py`:\n",
    "\n",
    "- Periodically retrieve the job status\n",
    "- Format the output into a table\n",
    "- When any evaluations get completed, it fetches detailed results from the NeMo Evaluator Microservice, and uploads them to MLflow for visualization.\n",
    "\n",
    "This makes it easier to compare and analyze the results. \n",
    "\n",
    "### (Optional) Viewing the MLflow dashboard\n",
    "If MLflow visualization is enabled, the MLflow dashboard will be available at port 5000 (default)\n",
    "* **If using Brev Launchable:**\n",
    "    Navigate to your Brev instance page, go to the `Access` tab, select `Using Secure Links`, and click the link that looks like `https://mlflow-*.brevlab.com`. As evaluation jobs complete, they will be logged in MLflow with their flywheel `job_id` as the MLflow experiment name.\n",
    "* **If using a Self-Hosted Notebook Setup:**\n",
    "    Open your browser and go to `<local-IP>:5000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Continuous Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The first customization run typically takes about **10 minutes** to start while the training container is being downloaded. The `monitor_job` call in the cell below usually requires around **50 minutes** to complete training and evaluation of the candidate NIMs, though the exact duration may vary depending on the specific GPU and the responsiveness of the remote endpoint used for LLM-judge evaluations.  \n",
    ">  \n",
    "> As the evaluations complete, you will begin to see metrics appear in both the table and the MLflow dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.job_monitor_helper import monitor_job\n",
    "\n",
    "monitor_job(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    job_id=job_id,\n",
    "    poll_interval=5\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youâ€™ve now successfully completed a Flywheel run and can review the evaluation results to decide whether to promote the customized or ICL model. However, with only 300 data points, the customized `Llama-3.2-1B-instruct` is likely still limited in accuracy.\n",
    "\n",
    "That said, the Data Flywheel operates as a self-reinforcing cycleâ€”models continue to improve as more user interaction logs are collected. Below, we demonstrate how the model accuracy improves incrementally with additional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Show Continuous Improvement (Optional)\n",
    "\n",
    "To extend the flywheel run with additional data, weâ€™ll launch a new job using `client_id` set to \"aiva-2\", which includes **500** data points, to evaluate the impact of increased data volume on performance.\n",
    "\n",
    "Note that `client_id` is originally intended to identify the client that generated the traffic. However, in the notebook, it was repurposed to represent datasets of varying sizes, illustrating the progressive improvement of the data flywheel as more data is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-2\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_job(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    job_id=job_id,\n",
    "    poll_interval=5\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some improvements of the customized model compared to the last run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have now collected even more data points, let's kick off another flywheel run by setting `client_id` to \"aiva-3\" which includes **1,000** records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-3\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_job(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    job_id=job_id,\n",
    "    poll_interval=5\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run with 1,000 data points, we should observe the customized modelâ€™s accuracies improving significantly, with the Function name accuracy approaching 1.0. \n",
    "\n",
    "This indicates that the customized `LLama-3.2-1B-instruct` model achieves accuracy comparable to the much larger `LLama-3.3-70B-instruct` base model deployed in AI Virtual Assistant, while significantly reducing latency and compute usage thanks to its smaller size. \n",
    "\n",
    "In the next step, we will show how to deploy the customized `LLama-3.2-1B-instruct` and run inference with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Deploy Customized Model and Run Inference\n",
    "\n",
    "NeMo Microsevices Platform provides two component microservices to simplify model deployment and inference:\n",
    "\n",
    "- NeMo Deployment Management: Provides an API to deploy NIM on a Kubernetes cluster and manage them through the NIM Operator microservice.\n",
    "- NeMo NIM Proxy: Provides a unified endpoint that you can use to access all deployed NIM for inference tasks.\n",
    "\n",
    "First, we need to deploy the base `LLama-3.2-1B-instruct` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://nemo.test/v1/deployment/model-deployments\"  # if you used Launchable setup, then the base url will be http://nemo.test. Otherwise, change the base url accordingly.\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "payload = {\n",
    "    \"name\": \"llama-3.2-1b-instruct\",\n",
    "    \"namespace\": \"meta\",\n",
    "    \"config\": {\n",
    "        \"model\": \"meta/llama-3.2-1b-instruct\",\n",
    "        \"nim_deployment\": {\n",
    "            \"image_name\": \"nvcr.io/nim/meta/llama-3.2-1b-instruct\",\n",
    "            \"image_tag\": \"1.8\",\n",
    "            \"pvc_size\": \"25Gi\",\n",
    "            \"gpu\": 1,\n",
    "            \"additional_envs\": {\n",
    "                \"NIM_GUIDED_DECODING_BACKEND\": \"outlines\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get the name of the fine-tuned model, which can be obtained from the job status API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f\"{API_BASE_URL}/api/jobs/{job_id}\")  # use the job_id for the best run based on eval results\n",
    "ft_model_name = response.json()['nims'][0]['customizations'][0]['customized_model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run inference with the fine-tuned model using the NIM Proxy service:\n",
    "Note: Wait until the deployment is finished, it may take ~5 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# get a example input request for inference\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "random_line = random.choice(lines)\n",
    "input_data = json.loads(random_line)\n",
    "\n",
    "url = \"http://nim.test/v1/chat/completions\"  # if you used Launchable setup, then the base url will be http://nim.test. Otherwise, change the base url accordingly.\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"model\": ft_model_name,\n",
    "    \"messages\": input_data['request']['messages'],\n",
    "    \"tools\": input_data['request']['tools'],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "response.json()['choices'][0]['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can also download the LoRA adapters. In Nemo Microservices Platform, LoRA adapters are uploaded to NeMo Data Store after fine tuning. NeMo Data Store exposing APIs compatible with the Hugging Face Hub client which we can interact with to download the LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "DATA_STORE_ENDPOINT = \"http://data-store.test\"  # if you used Launchable setup, then the base url to NeMo Data Store will be http://data-store.test. Otherwise, change the base url accordingly.\n",
    "MODEL_ID, CUSTOMIZATION_JOB_ID = ft_model_name.split('@', 1)\n",
    "\n",
    "# Download the model\n",
    "local_model_path = snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    repo_type=\"model\",\n",
    "    revision=CUSTOMIZATION_JOB_ID,\n",
    "    local_dir=\"./downloaded_loras\",\n",
    "    endpoint=f\"{DATA_STORE_ENDPOINT}/v1/hf\",\n",
    "    force_download=True\n",
    ")\n",
    "\n",
    "print(f\"Model downloaded to: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Expose the Customized Model for External Access (Brev Launchable Only)\n",
    "\n",
    "To enable external access to the customized model, we first need to do local port forwarding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Forward local port 8080 to service port 8000\n",
    "log_file = open(\"port_forward.log\", \"w\")\n",
    "pf_process = subprocess.Popen(\n",
    "    [\"kubectl\", \"port-forward\", \"svc/nemo-nim-proxy\", \"8080:8000\"],\n",
    "    stdout=log_file,\n",
    "    stderr=log_file,\n",
    ")\n",
    "\n",
    "print(f\"Port-forward started in background with PID {pf_process.pid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next navigate to your Brev instance page, go to the `Access` tab, select `Using Secure Links`, find the link that looks like https://inference-*.brevlab.com, click the `Edit Access`, and toggle on `Make Public`.\n",
    "\n",
    "Now you should be able to run inference with the above url externally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://inference-16qlcb1wy.brevlab.com/v1/chat/completions\"  # if you used Launchable setup, update the url according to the url from Brev instance page.\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"model\": ft_model_name,\n",
    "    \"messages\": input_data['request']['messages'],\n",
    "    \"tools\": input_data['request']['tools'],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "response.json()['choices'][0]['message']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
